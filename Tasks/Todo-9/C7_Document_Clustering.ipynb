{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "This section clones a repository (nlp-snippets) from GitHub, which may contain pre-written NLP code or utilities for the project.\n",
        "The pwd command prints the current directory to verify the working directory.\n"
      ],
      "metadata": {
        "id": "NXn6-PvM13Pk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oi000a62xANw",
        "outputId": "7910f28e-24ae-4a17-cad1-9c1aad7e562e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'nlp-snippets'...\n",
            "remote: Enumerating objects: 126, done.\u001b[K\n",
            "remote: Counting objects: 100% (126/126), done.\u001b[K\n",
            "remote: Compressing objects: 100% (96/96), done.\u001b[K\n",
            "remote: Total 126 (delta 54), reused 82 (delta 23), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (126/126), 2.66 MiB | 9.61 MiB/s, done.\n",
            "Resolving deltas: 100% (54/54), done.\n"
          ]
        }
      ],
      "source": [
        "! git clone https://github.com/dylanjcastillo/nlp-snippets.git\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! cd nlp-snippets/"
      ],
      "metadata": {
        "id": "wDxD-T5AxN46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hxxJ4ts0yZmr",
        "outputId": "a29e0426-81a5-49e2-812c-a7370dc3c916"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/nlp-snippets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import re\n",
        "import string\n",
        "\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from sklearn.cluster import MiniBatchKMeans\n",
        "from sklearn.metrics import silhouette_samples, silhouette_score\n",
        "\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"punkt\")\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
        "np.random.seed(SEED)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ldh_0EusxEPd",
        "outputId": "37294821-bd83-4b4b-cb79-26d4e3670f2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "os, random, re, string: Used for file handling, randomization, and text preprocessing.\n",
        "nltk: Natural language toolkit for text processing (tokenization, stopwords).\n",
        "numpy & pandas: Used for numerical operations and handling data in tabular form (dataframes).\n",
        "gensim: Provides word embedding models like Word2Vec for creating word vector representations.\n",
        "scikit-learn: Contains clustering algorithms (e.g., MiniBatchKMeans) and evaluation metrics like silhouette scores."
      ],
      "metadata": {
        "id": "tm-r2zyH18W5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text, tokenizer, stopwords):\n",
        "    \"\"\"Pre-process text and generate tokens\n",
        "\n",
        "    Args:\n",
        "        text: Text to tokenize.\n",
        "\n",
        "    Returns:\n",
        "        Tokenized text.\n",
        "    \"\"\"\n",
        "    text = str(text).lower()  # Lowercase words\n",
        "    text = re.sub(r\"\\[(.*?)\\]\", \"\", text)  # Remove [+XYZ chars] in content\n",
        "    text = re.sub(r\"\\s+\", \" \", text)  # Remove multiple spaces in content\n",
        "    text = re.sub(r\"\\w+…|…\", \"\", text)  # Remove ellipsis (and last word)\n",
        "    text = re.sub(r\"(?<=\\w)-(?=\\w)\", \" \", text)  # Replace dash between words\n",
        "    text = re.sub(\n",
        "        f\"[{re.escape(string.punctuation)}]\", \"\", text\n",
        "    )  # Remove punctuation\n",
        "\n",
        "    tokens = tokenizer(text)  # Get tokens from text\n",
        "    tokens = [t for t in tokens if not t in stopwords]  # Remove stopwords\n",
        "    tokens = [\"\" if t.isdigit() else t for t in tokens]  # Remove digits\n",
        "    tokens = [t for t in tokens if len(t) > 1]  # Remove short tokens\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "RnpEV38NxPZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tbZbS9oDyTGj",
        "outputId": "98cb6bcf-5e0b-4ed6-cb7f-f6e841a79d91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/nlp-snippets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import re\n",
        "import string\n",
        "\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from sklearn.cluster import MiniBatchKMeans\n",
        "from sklearn.metrics import silhouette_samples, silhouette_score\n",
        "\n",
        "# Download necessary NLTK data packages\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download('punkt_tab') # Download the missing punkt_tab data\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
        "np.random.seed(SEED)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VJs64GtCyjw7",
        "outputId": "50945ccc-968c-4e19-c74d-c3ff16a93ee5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Downloads NLTK resources (stopwords and Punkt tokenizer) for text processing.\n"
      ],
      "metadata": {
        "id": "odTue35d2BOm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Purpose: Pre-processes and tokenizes the text.\n",
        "\n",
        "Steps:\n",
        "\n",
        "Converts the text to lowercase.\n",
        "Removes unwanted characters such as numbers, punctuation, special characters, and extra spaces.\n",
        "Tokenizes the text using the specified tokenizer (e.g., NLTK word_tokenize).\n",
        "Removes stopwords.\n",
        "Filters out short tokens and digits.\n",
        "Returns: A list of cleaned tokens"
      ],
      "metadata": {
        "id": "8nJ8umoF2Mkl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "custom_stopwords = set(stopwords.words(\"english\") + [\"news\", \"new\", \"top\"])\n",
        "text_columns = [\"title\", \"description\", \"content\"]\n",
        "\n",
        "df_raw = pd.read_csv(\"data/news_data.csv\")\n",
        "df = df_raw.copy()\n",
        "df[\"content\"] = df[\"content\"].fillna(\"\")\n",
        "\n",
        "for col in text_columns:\n",
        "    df[col] = df[col].astype(str)\n",
        "\n",
        "# Create text column based on title, description, and content\n",
        "df[\"text\"] = df[text_columns].apply(lambda x: \" | \".join(x), axis=1)\n",
        "df[\"tokens\"] = df[\"text\"].map(lambda x: clean_text(x, word_tokenize, custom_stopwords))\n",
        "\n",
        "# Remove duplicated after preprocessing\n",
        "_, idx = np.unique(df[\"tokens\"], return_index=True)\n",
        "df = df.iloc[idx, :]\n",
        "\n",
        "# Remove empty values and keep relevant columns\n",
        "df = df.loc[df.tokens.map(lambda x: len(x) > 0), [\"text\", \"tokens\"]]\n",
        "\n",
        "docs = df[\"text\"].values\n",
        "tokenized_docs = df[\"tokens\"].values\n",
        "\n",
        "print(f\"Original dataframe: {df_raw.shape}\")\n",
        "print(f\"Pre-processed dataframe: {df.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IL8X8L7OxTDG",
        "outputId": "27e03566-adca-49ee-e1d2-ccd22ea31660"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original dataframe: (10437, 15)\n",
            "Pre-processed dataframe: (9882, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ensures reproducibility of results by setting a fixed random seed for various operations (e.g., shuffling, initializing models)."
      ],
      "metadata": {
        "id": "gMtHWwb-2FeK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trains a Word2Vec model on the tokenized documents with 100-dimensional vectors.\n",
        "The Word2Vec model learns the vector representations of words based on their co-occurrence in the documents."
      ],
      "metadata": {
        "id": "SFvP_uol2Q3i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Word2Vec(sentences=tokenized_docs, vector_size=100, workers=1, seed=SEED)"
      ],
      "metadata": {
        "id": "ijxYUgBdyoYx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "wv = api.load('word2vec-google-news-300')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ExqFdHsMyvKn",
        "outputId": "fd7c61f3-05de-40ec-e0d0-738706c97467"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[=============================================-----] 90.5% 1504.2/1662.8MB downloaded"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.wv.most_similar(\"trump\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yM7Dk81UzKxA",
        "outputId": "3f45ced9-a6ed-4975-8c05-401c85fe5641"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('trumps', 0.9885427355766296),\n",
              " ('president', 0.9746479988098145),\n",
              " ('donald', 0.9274885654449463),\n",
              " ('ivanka', 0.9203841686248779),\n",
              " ('impeachment', 0.9195799827575684),\n",
              " ('pences', 0.9152251482009888),\n",
              " ('avlon', 0.9148187637329102),\n",
              " ('biden', 0.9145993590354919),\n",
              " ('breitbart', 0.9144167900085449),\n",
              " ('vice', 0.9067206978797913)]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Purpose: Converts the list of documents (tokenized) into vectors using the word embeddings.\n",
        "Steps:\n",
        "For each document, it creates a vector by averaging the vectors of the words in that document.\n",
        "If no words in a document have embeddings (i.e., not in the model's vocabulary), it returns a zero vector."
      ],
      "metadata": {
        "id": "zcJ2QWMf2V4o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorize(list_of_docs, model):\n",
        "    \"\"\"Generate vectors for list of documents using a Word Embedding\n",
        "\n",
        "    Args:\n",
        "        list_of_docs: List of documents\n",
        "        model: Gensim's Word Embedding\n",
        "\n",
        "    Returns:\n",
        "        List of document vectors\n",
        "    \"\"\"\n",
        "    features = []\n",
        "\n",
        "    for tokens in list_of_docs:\n",
        "        zero_vector = np.zeros(model.vector_size)\n",
        "        vectors = []\n",
        "        for token in tokens:\n",
        "            if token in model.wv:\n",
        "                try:\n",
        "                    vectors.append(model.wv[token])\n",
        "                except KeyError:\n",
        "                    continue\n",
        "        if vectors:\n",
        "            vectors = np.asarray(vectors)\n",
        "            avg_vec = vectors.mean(axis=0)\n",
        "            features.append(avg_vec)\n",
        "        else:\n",
        "            features.append(zero_vector)\n",
        "    return features\n",
        "\n",
        "vectorized_docs = vectorize(tokenized_docs, model=model)\n",
        "len(vectorized_docs), len(vectorized_docs[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QEL8Ujo83Qzl",
        "outputId": "28918ef9-8313-4717-c019-45f3bfae15ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(9882, 100)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Purpose: Performs clustering using MiniBatchKMeans, which is efficient for large datasets.\n",
        "Steps:\n",
        "Applies MiniBatchKMeans with a specified number of clusters (k) and mini-batch size (mb).\n",
        "Optionally prints silhouette scores for evaluating the quality of clusters.\n",
        "Prints out silhouette scores for each cluster to assess how well the clustering algorithm performed."
      ],
      "metadata": {
        "id": "E884d3iL2Z7L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mbkmeans_clusters(\n",
        "    X,\n",
        "    k,\n",
        "    mb,\n",
        "    print_silhouette_values,\n",
        "):\n",
        "    \"\"\"Generate clusters and print Silhouette metrics using MBKmeans\n",
        "\n",
        "    Args:\n",
        "        X: Matrix of features.\n",
        "        k: Number of clusters.\n",
        "        mb: Size of mini-batches.\n",
        "        print_silhouette_values: Print silhouette values per cluster.\n",
        "\n",
        "    Returns:\n",
        "        Trained clustering model and labels based on X.\n",
        "    \"\"\"\n",
        "    km = MiniBatchKMeans(n_clusters=k, batch_size=mb).fit(X)\n",
        "    print(f\"For n_clusters = {k}\")\n",
        "    print(f\"Silhouette coefficient: {silhouette_score(X, km.labels_):0.2f}\")\n",
        "    print(f\"Inertia:{km.inertia_}\")\n",
        "\n",
        "    if print_silhouette_values:\n",
        "        sample_silhouette_values = silhouette_samples(X, km.labels_)\n",
        "        print(f\"Silhouette values:\")\n",
        "        silhouette_values = []\n",
        "        for i in range(k):\n",
        "            cluster_silhouette_values = sample_silhouette_values[km.labels_ == i]\n",
        "            silhouette_values.append(\n",
        "                (\n",
        "                    i,\n",
        "                    cluster_silhouette_values.shape[0],\n",
        "                    cluster_silhouette_values.mean(),\n",
        "                    cluster_silhouette_values.min(),\n",
        "                    cluster_silhouette_values.max(),\n",
        "                )\n",
        "            )\n",
        "        silhouette_values = sorted(\n",
        "            silhouette_values, key=lambda tup: tup[2], reverse=True\n",
        "        )\n",
        "        for s in silhouette_values:\n",
        "            print(\n",
        "                f\"    Cluster {s[0]}: Size:{s[1]} | Avg:{s[2]:.2f} | Min:{s[3]:.2f} | Max: {s[4]:.2f}\"\n",
        "            )\n",
        "    return km, km.labels_"
      ],
      "metadata": {
        "id": "0ab1SExD3Uva"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clustering, cluster_labels = mbkmeans_clusters(\n",
        "    X=vectorized_docs,\n",
        "    k=50,\n",
        "    mb=500,\n",
        "    print_silhouette_values=True,\n",
        ")\n",
        "df_clusters = pd.DataFrame({\n",
        "    \"text\": docs,\n",
        "    \"tokens\": [\" \".join(text) for text in tokenized_docs],\n",
        "    \"cluster\": cluster_labels\n",
        "})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "btUxwciu3YAi",
        "outputId": "2893ddbc-9df8-4161-a84c-b79db3d4791b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For n_clusters = 50\n",
            "Silhouette coefficient: 0.11\n",
            "Inertia:3558.382223620125\n",
            "Silhouette values:\n",
            "    Cluster 42: Size:31 | Avg:0.33 | Min:0.04 | Max: 0.53\n",
            "    Cluster 4: Size:100 | Avg:0.32 | Min:-0.16 | Max: 0.52\n",
            "    Cluster 36: Size:145 | Avg:0.27 | Min:-0.04 | Max: 0.51\n",
            "    Cluster 16: Size:110 | Avg:0.25 | Min:-0.02 | Max: 0.44\n",
            "    Cluster 43: Size:85 | Avg:0.25 | Min:-0.02 | Max: 0.44\n",
            "    Cluster 34: Size:80 | Avg:0.24 | Min:-0.00 | Max: 0.44\n",
            "    Cluster 25: Size:35 | Avg:0.24 | Min:0.02 | Max: 0.43\n",
            "    Cluster 11: Size:137 | Avg:0.24 | Min:-0.03 | Max: 0.45\n",
            "    Cluster 33: Size:60 | Avg:0.23 | Min:-0.06 | Max: 0.46\n",
            "    Cluster 24: Size:67 | Avg:0.22 | Min:-0.27 | Max: 0.47\n",
            "    Cluster 44: Size:45 | Avg:0.21 | Min:-0.02 | Max: 0.41\n",
            "    Cluster 1: Size:127 | Avg:0.21 | Min:-0.03 | Max: 0.41\n",
            "    Cluster 41: Size:68 | Avg:0.21 | Min:-0.08 | Max: 0.43\n",
            "    Cluster 35: Size:81 | Avg:0.20 | Min:-0.00 | Max: 0.41\n",
            "    Cluster 17: Size:254 | Avg:0.20 | Min:-0.05 | Max: 0.39\n",
            "    Cluster 7: Size:138 | Avg:0.16 | Min:-0.06 | Max: 0.40\n",
            "    Cluster 5: Size:121 | Avg:0.16 | Min:-0.05 | Max: 0.38\n",
            "    Cluster 31: Size:64 | Avg:0.15 | Min:-0.03 | Max: 0.37\n",
            "    Cluster 21: Size:535 | Avg:0.15 | Min:-0.02 | Max: 0.36\n",
            "    Cluster 29: Size:540 | Avg:0.15 | Min:-0.05 | Max: 0.36\n",
            "    Cluster 10: Size:90 | Avg:0.15 | Min:-0.10 | Max: 0.39\n",
            "    Cluster 3: Size:146 | Avg:0.14 | Min:-0.03 | Max: 0.30\n",
            "    Cluster 20: Size:225 | Avg:0.13 | Min:-0.09 | Max: 0.36\n",
            "    Cluster 46: Size:329 | Avg:0.13 | Min:-0.11 | Max: 0.36\n",
            "    Cluster 30: Size:413 | Avg:0.12 | Min:-0.07 | Max: 0.32\n",
            "    Cluster 39: Size:152 | Avg:0.12 | Min:-0.05 | Max: 0.34\n",
            "    Cluster 27: Size:144 | Avg:0.11 | Min:-0.11 | Max: 0.36\n",
            "    Cluster 32: Size:117 | Avg:0.11 | Min:-0.07 | Max: 0.32\n",
            "    Cluster 13: Size:276 | Avg:0.10 | Min:-0.12 | Max: 0.35\n",
            "    Cluster 40: Size:118 | Avg:0.10 | Min:-0.12 | Max: 0.28\n",
            "    Cluster 28: Size:97 | Avg:0.10 | Min:-0.20 | Max: 0.38\n",
            "    Cluster 9: Size:475 | Avg:0.10 | Min:-0.09 | Max: 0.28\n",
            "    Cluster 37: Size:339 | Avg:0.10 | Min:-0.03 | Max: 0.29\n",
            "    Cluster 26: Size:236 | Avg:0.09 | Min:-0.13 | Max: 0.30\n",
            "    Cluster 49: Size:179 | Avg:0.09 | Min:-0.07 | Max: 0.26\n",
            "    Cluster 12: Size:580 | Avg:0.09 | Min:-0.08 | Max: 0.27\n",
            "    Cluster 8: Size:88 | Avg:0.09 | Min:-0.11 | Max: 0.31\n",
            "    Cluster 14: Size:266 | Avg:0.08 | Min:-0.08 | Max: 0.30\n",
            "    Cluster 18: Size:199 | Avg:0.08 | Min:-0.10 | Max: 0.30\n",
            "    Cluster 23: Size:220 | Avg:0.08 | Min:-0.11 | Max: 0.32\n",
            "    Cluster 38: Size:256 | Avg:0.07 | Min:-0.12 | Max: 0.26\n",
            "    Cluster 45: Size:193 | Avg:0.07 | Min:-0.14 | Max: 0.29\n",
            "    Cluster 2: Size:180 | Avg:0.07 | Min:-0.15 | Max: 0.32\n",
            "    Cluster 22: Size:96 | Avg:0.07 | Min:-0.13 | Max: 0.29\n",
            "    Cluster 19: Size:165 | Avg:0.04 | Min:-0.14 | Max: 0.26\n",
            "    Cluster 15: Size:273 | Avg:0.03 | Min:-0.22 | Max: 0.26\n",
            "    Cluster 48: Size:144 | Avg:0.02 | Min:-0.18 | Max: 0.24\n",
            "    Cluster 6: Size:484 | Avg:0.02 | Min:-0.17 | Max: 0.26\n",
            "    Cluster 47: Size:284 | Avg:0.01 | Min:-0.21 | Max: 0.25\n",
            "    Cluster 0: Size:295 | Avg:0.01 | Min:-0.19 | Max: 0.22\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Most representative terms per cluster (based on centroids):\")\n",
        "for i in range(50):\n",
        "    tokens_per_cluster = \"\"\n",
        "    most_representative = model.wv.most_similar(positive=[clustering.cluster_centers_[i]], topn=5)\n",
        "    for t in most_representative:\n",
        "        tokens_per_cluster += f\"{t[0]} \"\n",
        "    print(f\"Cluster {i}: {tokens_per_cluster}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OtgdLrC73dIn",
        "outputId": "3296408f-d676-4628-9129-02e9dac86d30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most representative terms per cluster (based on centroids):\n",
            "Cluster 0: december lawsuits manhattan decided baker \n",
            "Cluster 1: leo delay jo mps referendum \n",
            "Cluster 2: professional edition expensive popular performance \n",
            "Cluster 3: obama emmanuel impeach whistleblowers congress \n",
            "Cluster 4: category humberto landfall charleston wrath \n",
            "Cluster 5: stabbing murdering neighbor convicted manslaughter \n",
            "Cluster 6: supply yields wireless flagship managers \n",
            "Cluster 7: hospital bomb dozens soldiers injuring \n",
            "Cluster 8: geneva escalation assembly countrys italy \n",
            "Cluster 9: prize amount walk formula born \n",
            "Cluster 10: proposal compromise 31st impasse reject \n",
            "Cluster 11: serial passenger shocked contained conducted \n",
            "Cluster 12: orleans television opens produced mayo \n",
            "Cluster 13: agree imran alliance abu deadline \n",
            "Cluster 14: regulator rome automaker berlin geneva \n",
            "Cluster 15: appearances 20th haul mcavoy april \n",
            "Cluster 16: squad qualifying warm foursomes finals \n",
            "Cluster 17: likes tips deals computers coffee \n",
            "Cluster 18: representative pushed argued ursula opposing \n",
            "Cluster 19: aides ukrainian vizcarra congressional volodymyr \n",
            "Cluster 20: dealt jodie edinburgh tafida beds \n",
            "Cluster 21: keeping complete draft evil thanks \n",
            "Cluster 22: treasury moscow environmental watchdog kiev \n",
            "Cluster 23: arrest targeting brazilian french illinois \n",
            "Cluster 24: forecasters trajectory slams sharpie claim \n",
            "Cluster 25: clashes protesters defied erupted demonstrators \n",
            "Cluster 26: indiana father custody assaulting rape \n",
            "Cluster 27: tariff flows lift slap imports \n",
            "Cluster 28: ai popularity linkedin content elon \n",
            "Cluster 29: success usually fitness actually edge \n",
            "Cluster 30: fair planned file previously citizens \n",
            "Cluster 31: strikes plants gas ablaze tanker \n",
            "Cluster 32: responds conversation taxpayers staying messing \n",
            "Cluster 33: cnns interested transcript ridiculed contributor \n",
            "Cluster 34: cnnpolitics putin complaint clinton pences \n",
            "Cluster 35: clearing unrest exchanges mainland defied \n",
            "Cluster 36: islands charleston flooding ravaged coastal \n",
            "Cluster 37: january goals liverpool summer winning \n",
            "Cluster 38: dept peru subpoenas steven assad \n",
            "Cluster 39: dominic suspend block speech swinson \n",
            "Cluster 40: rudy barack clinton giuliani hillary \n",
            "Cluster 41: jury officers pleaded amber neighbour \n",
            "Cluster 42: eye path lilinow halts projected \n",
            "Cluster 43: proposals johnsons backstop pm benjamin \n",
            "Cluster 44: arabian yemens yemen houthi tanker \n",
            "Cluster 45: tournament finished qualify victory madrid \n",
            "Cluster 46: relatives accident pulled ohio flat \n",
            "Cluster 47: collapsed shut libya dropped command \n",
            "Cluster 48: test winner bowl scored wickets \n",
            "Cluster 49: tweet insults panel cnnin strategist \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Purpose: Identifies the most representative documents for a specific cluster (e.g., test_cluster = 29).\n",
        "Method: Measures the Euclidean distance between each document vector and the centroid of the cluster, then sorts documents by closeness to the cluster's centroid.\n",
        "Output: Displays the top 3 most representative documents for the chosen cluster."
      ],
      "metadata": {
        "id": "YOB6SN9R2dEu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_cluster = 29\n",
        "most_representative_docs = np.argsort(\n",
        "    np.linalg.norm(vectorized_docs - clustering.cluster_centers_[test_cluster], axis=1)\n",
        ")\n",
        "for d in most_representative_docs[:3]:\n",
        "    print(docs[d])\n",
        "    print(\"-------------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q4fXC4b03erW",
        "outputId": "fd947976-bdf3-4857-e32c-c5a14e3eb439"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Netflix's 'Unbelievable' Is True Crime Meets 'True Detective', Perfect For 'Mindhunter' Fans | The limited series will premiere on September 13. | A new Netflix original series will dive deep into the trauma and legalities that come with a rape accusation. The emotional show, Unbelievable, will air on the streaming platform on September 13 as a limited series. Reviews are already calling it a true crime… [+2186 chars]\n",
            "-------------\n",
            "How Fast Fashion Is Destroying the Planet | In “Fashionopolis,” Dana Thomas exposes the environmental, economic and humanitarian hazards of cheap clothing production. | Among the books delights are Thomass sketches of her individual subjects. I cant get her description of a woman as peaches-and-cream pretty out of my head; I know exactly what she looks like. The author also has a gift for bringing luxury to life: She conjure… [+3349 chars]\n",
            "-------------\n",
            "The work of beloved TV artist Bob Ross is finally being recognized in an exhibition | If you've ever wanted to see some of Bob Ross' \"happy little trees\" in person, now's your chance. | artsPublished 10th September 2019\n",
            "\"Happy little clouds,\" \"happy little trees,\" \"happy little leaves.\" The late American painter and beloved TV host Bob Ross had a unique and blissful way of teaching art.\n",
            "Ross' how-to painting show, \"The Joy of Painting,\" wa… [+7010 chars]\n",
            "-------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conclusion\n",
        "The code demonstrates a typical document clustering pipeline, starting with text preprocessing, generating word embeddings with Word2Vec, and using MiniBatchKMeans for efficient clustering.\n",
        "Evaluation is performed using silhouette scores, and the code also offers insight into the most representative terms and documents for each cluster."
      ],
      "metadata": {
        "id": "ZAseUxwM2fZJ"
      }
    }
  ]
}